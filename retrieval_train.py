# -*- coding: utf-8 -*-
"""empty_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXpWi0DHhc07-pW_hofyqDc2xtUru1Fv
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install --quiet scann==1.2.8
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras import Model,layers,Input
from tensorflow.keras.layers import *
from HelperFunctions import time_conv,time_sorter,time_splitter,last_n_taker,unique_definer,corpus_creator,sequencer_multi,sequencer_unique,input_label_maker
from HelperFunctions import train_neg_maker,release_year
from metrics import mapk
from models import ret_model
import os 
from sklearn.utils import shuffle
import pickle
import scann

EMBEDDING_DIM = 256
SEQUENCE_LEN = 20
BATCH_SIZE = 512
NUM_EPOCHS = 100
K = 40

def pickle_dumper(dictin,dictin_name):
    name_str = dictin_name+'.pkl'
    a_file = open(name_str, "wb")
    pickle.dump(dictin, a_file)
    a_file.close()

saving_path = 'drive/MyDrive/Colab Notebooks/two_stage_rec_2/'

#### FOR THIS PROJECT, I WILL USE THE MOVIELENS 1M DATASET, BECAUSE 1M DATASET IS USER REACH FEATURES
movie_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/movies.dat'
movies_df = pd.read_csv(movie_url, delimiter = '::',encoding='ISO-8859-1',header = None)
movies_df.columns = ['movie_id','movie_title','genres']

ratings_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/ratings.dat'
ratings_df = pd.read_csv(ratings_url, delimiter = '::', header = None)
ratings_df.columns = ['user_id','movie_id','ratings','timestamp']

user_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/ml_1M/users.dat'
users_df = pd.read_csv(user_url, delimiter = '::', header = None)
users_df.columns = ['user_id','gender', 'age', 'occupation', 'zip_code']

print('movies_df LENGTH :',len(movies_df))
print('ratings_df LENGTH :',len(ratings_df))
print('users_df LENGTH :',len(users_df))

# print(len(ratings_df.movie_id.unique()))
# print(len(ratings_df['movie_id'].unique()))

## all datasets will be merged and final dataset will named as retrieval_df
ratings_df = movies_df.merge(ratings_df,left_on='movie_id',right_on='movie_id')
retrieval_df = ratings_df.merge(users_df, left_on = 'user_id', right_on = 'user_id')

retrieval_df['timestamp'] = time_conv(retrieval_df['timestamp'])
retrieval_df = time_sorter(retrieval_df,'user_id', 'timestamp')

print('number unique movie title at movies_df :',len(movies_df['movie_title'].unique()))
print('number unique movie title at movies_df :',len(movies_df['movie_id'].unique()))

print('minimum number of movies that were interacted with user',np.min(retrieval_df.groupby(['user_id'])['movie_id'].count()))
print('maximum number of movies that were interacted with user',np.max(retrieval_df.groupby(['user_id'])['movie_id'].count()))
print(retrieval_df.info())
retrieval_df.head(3)

retrieval_df_diluted = last_n_taker(retrieval_df, 'user_id', SEQUENCE_LEN)

print('length of the retrieval_df_diluted (every user has same amount of movie) :',len(retrieval_df_diluted))

### finding unique entries for each features, this part will be used for creating corpus.
unique_users, num_unique_users = unique_definer(retrieval_df_diluted,'user_id')
unique_movies, num_unique_movies = unique_definer(retrieval_df_diluted,'movie_id')
unique_occupations, num_unique_occupations = unique_definer(retrieval_df_diluted, 'occupation')
unique_zips, num_unique_zips = unique_definer(retrieval_df_diluted, 'zip_code')

### creating corpus for each feature 
### for some features label encoders will be employed
#**************BU KISIMDA KI ENCODER SOZLUKLERI DEGISECEK*******************
user_2enc, enc_2user = corpus_creator(unique_users, start_index = 0)
movie_2enc, enc_2movie = corpus_creator(unique_movies, start_index = 0)
occ_2enc, enc_2occ = corpus_creator(unique_occupations, start_index = 0)
zip_2enc, enc_2zip = corpus_creator(unique_zips, start_index = 0)
mid_2title = { retrieval_df_diluted['movie_id'][i]:retrieval_df_diluted['movie_title'][i] for i in range(len(retrieval_df_diluted))}
title_2mid = { retrieval_df_diluted['movie_title'][i]:retrieval_df_diluted['movie_id'][i] for i in range(len(retrieval_df_diluted))}
#**************BU KISIMDA KI ENCODER SOZLUKLERI DEGISECEK*******************
LE_gender = LabelEncoder()
LE_age = LabelEncoder()
LE_ge = LabelEncoder()

### for retrieval_df encoded version for each feature will be represented as retrieval_df_enc
retrieval_df_enc = pd.DataFrame({
                                'user_id' : retrieval_df_diluted['user_id'] ,
                                'movie_id_enc' : retrieval_df_diluted['movie_id'].map(enc_2movie) ,
                                'occupation_enc' : retrieval_df_diluted['occupation'].map(enc_2occ) ,
                                'gender_enc' : LE_gender.fit_transform(retrieval_df_diluted['gender']) ,
                                'age_enc' : LE_age.fit_transform(retrieval_df_diluted['age']) ,
                                'zip_code_enc' : retrieval_df_diluted['zip_code'].map(enc_2zip) ,
                                'timestamp' : retrieval_df_diluted['timestamp'].copy()
                                })
print('ENCODED RETRIEVAL_DF :')
# print()
retrieval_df_enc.info();
#### movie_id_enc will sequenced as function of time using sequencer multi and sequencer_unique funcitons

seq_retrieval_df_enc = sequencer_multi(retrieval_df_enc[['user_id','movie_id_enc']],'user_id')
seq_retrieval_df_enc_2 = sequencer_unique(retrieval_df_enc,'user_id')
seq_retrieval_df_enc['occupation_enc'] = seq_retrieval_df_enc_2['occupation_enc']
seq_retrieval_df_enc['gender_enc'] = seq_retrieval_df_enc_2['gender_enc']
seq_retrieval_df_enc['age_enc'] = seq_retrieval_df_enc_2['age_enc']
seq_retrieval_df_enc['zip_code_enc'] = seq_retrieval_df_enc_2['zip_code_enc']

del seq_retrieval_df_enc_2

"""#DATA CONVERSION"""

#### sequenced movies will split into test and train set using leave one out methodology,
#### last watched movie will be designated as label item or label movie
last_movie_enc, input_movie_enc = input_label_maker(seq_retrieval_df_enc,'movie_id_enc',SEQUENCE_LEN, splitter = True)
seq_retrieval_df_enc['last_movie_enc'] = last_movie_enc
seq_retrieval_df_enc['input_movie_enc'] = input_movie_enc

#### for testing purpose dataset will be arraged as train and test sets 
train_df, test_df = train_test_split(seq_retrieval_df_enc, train_size = 0.96, test_size = 0.04, random_state = 42)

#### data types will be converted appropriate format for neural network
input_movie_enc_train = [np.array(i)  for i in (train_df['input_movie_enc'])]
last_movie_enc_train = [np.array(i)  for i in (train_df['last_movie_enc'])]
occupation_enc_train = [np.array(i)  for i in (train_df['occupation_enc'])]
gender_enc_train = [np.array(i)  for i in (train_df['gender_enc'])]
age_enc_train = [np.array(i)  for i in (train_df['age_enc'])]
zip_code_enc_train = [np.array(i)  for i in (train_df['zip_code_enc'])]

input_movie_enc_test = [np.array(i)  for i in (test_df['input_movie_enc'])]
last_movie_enc_test = [np.array(i)  for i in (test_df['last_movie_enc'])]
occupation_enc_test = [np.array(i)  for i in (test_df['occupation_enc'])]
gender_enc_test = [np.array(i)  for i in (test_df['gender_enc'])]
age_enc_test = [np.array(i)  for i in (test_df['age_enc'])]
zip_code_enc_test = [np.array(i)  for i in (test_df['zip_code_enc'])]

input_movie_enc = [list(i)  for i in (seq_retrieval_df_enc['input_movie_enc'])]
last_movie_enc = [np.array(i)  for i in (seq_retrieval_df_enc['last_movie_enc'])]
occupation_enc = [list(i)  for i in (seq_retrieval_df_enc['occupation_enc'])]
gender_enc = [list(i)  for i in (seq_retrieval_df_enc['gender_enc'])]
age_enc = [list(i)  for i in (seq_retrieval_df_enc['age_enc'])]
zip_code_enc = [list(i)  for i in (seq_retrieval_df_enc['zip_code_enc'])]

retrieval_model = ret_model(SEQUENCE_LEN,EMBEDDING_DIM,BATCH_SIZE,NUM_EPOCHS,num_unique_movies)
retrieval_model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer = 'Adam',
    metrics = ['accuracy'],
    )

# retrieval_model.fit(
#     [   tf.constant(input_movie_enc_train),
#         tf.constant(occupation_enc_train),
#         tf.constant(gender_enc_train),
#         tf.constant(age_enc_train),
#         tf.constant(zip_code_enc_train)],
#         tf.constant(last_movie_enc_train),
#      epochs = NUM_EPOCHS,
#     #  batch_size = BATCH_SIZE,
#     verbose = 1
#      )

### One can save either model or model weights. Since, model weight takes less space, I prefer to save model weights rather that whole model
ret_model_name = 'retrieval_model_weights.h5'

# retrieval_model.save_weights(saving_path +str(NUM_EPOCHS)+'epochs_'+ret_model_name)
retrieval_model.load_weights(saving_path +str(NUM_EPOCHS)+'epochs_'+ret_model_name)
# retrieval_model.load_weights('drive/MyDrive/Colab Notebooks/two_stage_rec/retrieval_weights.h5')

item_embeddings = ((retrieval_model.get_layer("softmax_layer").weights)[0])
user_emb_model = Model(inputs = retrieval_model.inputs , outputs = retrieval_model.get_layer('d3_layer').output )
user_embeddings = user_emb_model([
                                            tf.constant(input_movie_enc),
                                            tf.constant(occupation_enc),
                                            tf.constant(gender_enc),
                                            tf.constant(age_enc),
                                            tf.constant(zip_code_enc)
                                            ])

searcher = scann.scann_ops_pybind.builder(np.transpose(item_embeddings), K, "dot_product").tree(
    num_leaves=20, num_leaves_to_search=10, training_sample_size=2000
    ).score_ah(2, anisotropic_quantization_threshold=0.2
    ).reorder(100).build()

## create serialize target dir
# saving_path = 'drive/MyDrive/Colab Notebooks/two_stage_rec'
saving_path = 'drive/MyDrive/Colab Notebooks/two_stage_rec/searcher'
os.makedirs(saving_path, exist_ok=True)
# ## serialize the searcher
searcher.serialize(saving_path)

searcher = scann.scann_ops_pybind.load_searcher(saving_path)

top_decoded_items = []
topN_enc_item_id = []
user_id = 1
user_id_col = []
for i in user_embeddings:
    index, distance = searcher.search(np.array(i).ravel())
    topN_enc_item_id.append(index)
    user_id_col.append(np.full(len(index),user_id))
    user_id += 1

### bu kisimda ranking df icin dataset hazirliniyor
retrieval_final_df = pd.DataFrame({'user_id':np.concatenate(user_id_col) , 'movie_id_enc':np.concatenate(topN_enc_item_id)})
retrieval_final_df['user_id'] = retrieval_final_df['user_id']
retrieval_final_df['movie_id'] = retrieval_final_df['movie_id_enc'].map(movie_2enc)
ranking_df = retrieval_final_df.copy()
ranking_df = ranking_df.drop_duplicates()
ranking_df = ranking_df.rename(columns = {'movie_id':'item_id'})
ranking_df = ranking_df.drop_duplicates()
ranking_df = ranking_df.merge(movies_df, left_on='item_id', right_on= 'movie_id')
ranking_df = ranking_df.drop_duplicates().reset_index(drop= True)

# seq_retrieval_df_enc.to_csv('seq_retrieval_df_enc.csv')
# test_df.to_csv(saving_path+'test_df_retrieval.csv')

"""#EVALUATION"""

top_decoded_items = []
topN_enc_item_id = []
user_id = 1
user_id_col = []

for i in user_embeddings:
    index, distance = searcher.search(np.array(i).ravel())
    topN_enc_item_id.append(index)
    user_id_col.append(np.full(len(index),user_id))
    user_id += 1
print('MAP',mapk(np.array(last_movie_enc_test).reshape(-1,1), np.concatenate(topN_enc_item_id).reshape(-1,1), k = 20))

def _compute_precision_recall(targets, predictions, k):

    pred = predictions[:k]
    num_hit = len(set(pred).intersection(set(targets)))
    precision = float(num_hit) / len(pred)
    recall = float(num_hit) / len(targets)
    return precision, recall

N = [1,3,5,10,20]
for t in N:
    precisions = []
    recalls = []
    for i, _k in enumerate(topN_enc_item_id):
        precision, recall = _compute_precision_recall([int(last_movie_enc[i])], _k,t)
    # print(precision)
        precisions.append(precision)
        recalls.append(recall)

    print('precision @',t, np.mean(precisions))
    print('recalls @',t, np.mean(recalls))
    print(' ')

